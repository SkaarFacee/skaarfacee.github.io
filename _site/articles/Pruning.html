<!DOCTYPE html>



<html lang="en" class="h-100">

<head>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="I turn coffee into code, use tabs over spaces and never broke production.">

  <title>Personal Website</title>
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css" type="text/css"/>
  
  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css" type="text/css">

</head>


<body class="d-flex flex-column h-100">

  <main class="flex-shrink-0 container mt-5">
  <nav class="navbar navbar-expand-lg navbar-light">

  <a class="navbar-brand" href="/"><h5><b>Personal Website</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto">
      
      
        
      
        
      
        
          
        
      
        
          
            <a class="nav-item nav-link " href=" /projects/ ">Projects</a>
          
        
      
        
          
            <a class="nav-item nav-link  active " href=" /articles/ ">Blog</a>
          
        
      
        
          
        
      
        
          
            <a class="nav-item nav-link " href=" /about/ ">About</a>
          
        
      
        
          
            <a class="nav-item nav-link " href=" https://github.com/skaarfacee/ "><i class="fab fa-1x fa-github"></i></a>
          
        
      
    </div>
  </div>

</nav>
  <div class="col-lg-10 mx-auto mt-5 article">
  <h1><b>Pruning and Distilling Large Language Models - A Path to Efficient AI</b></h1>

<p class="article-metadata text-muted">
  15 February 2025 -  
  <b>
  
  
    4 mins read time
  
  </b>

  <br>

  
    Tags: 
    
    <a class="text-decoration-none no-underline" href="/articles/tags#blogs">
      <span class="tag badge badge-pill text-primary border border-primary">Blogs</span>
    </a>
    
  

</p>

<p><strong>Title:</strong>  <strong>Pruning and Distilling Large Language Models: A Path to Efficient AI</strong></p>

<h3 id="introduction">Introduction</h3>

<p>Large Language Models (LLMs) have become a dominant force in natural language processing (NLP). However, their massive size and resource-intensive nature make them costly to train and deploy. In response, researchers are exploring ways to create smaller, more efficient models that maintain strong language understanding while reducing computational demands.</p>

<p>One effective strategy is combining <strong>weight pruning</strong> and <strong>knowledge distillation</strong>. NVIDIA was among the first to demonstrate that this approach significantly improves efficiency while maintaining performance.</p>

<h3 id="benefits-of-pruning-and-distillation">Benefits of Pruning and Distillation</h3>

<p>Applying pruning and distillation together offers several advantages:</p>

<ul>
  <li>
    <p><strong>Higher accuracy:</strong> Model benchmarks show that smaller models created using these techniques can achieve up to a <strong>60% increase in MMLU scores</strong> (Massive Multitask Language Understanding).</p>
  </li>
  <li>
    <p><strong>Reduced training cost:</strong> Smaller models require fewer training tokens and iterations, cutting costs.</p>
  </li>
  <li>
    <p><strong>Lower compute requirements:</strong> Less memory and processing power needed for deployment.</p>
  </li>
  <li>
    <p><strong>Performance retention:</strong> Despite being smaller, models retain competitive performance compared to leading architectures like Mistral and Hemma.</p>
  </li>
</ul>

<hr />

<h2 id="understanding-pruning-and-distillation"><strong>Understanding Pruning and Distillation</strong></h2>

<h3 id="what-is-pruning"><strong>What is Pruning?</strong></h3>

<p>Pruning reduces the size of a model by removing less important components. There are two primary types:</p>

<ul>
  <li>
    <p><strong>Depth Pruning:</strong> Eliminates entire layers from the model, reducing overall model depth and computation cost.</p>
  </li>
  <li>
    <p><strong>Width Pruning:</strong> Removes specific neurons and attention heads within layers, making each layer more efficient while maintaining depth.</p>
  </li>
</ul>

<p>Pruning often requires some level of retraining to recover accuracy lost due to parameter reduction. The effectiveness of pruning depends on how well important versus redundant parameters are identified.</p>

<h4 id="advanced-pruning-techniques"><strong>Advanced Pruning Techniques</strong></h4>

<ul>
  <li>
    <p><strong>Structured vs. Unstructured Pruning:</strong> Structured pruning removes entire neurons or attention heads, whereas unstructured pruning removes individual weights, making optimization more challenging but sometimes more effective.</p>
  </li>
  <li>
    <p><strong>Lottery Ticket Hypothesis:</strong> Some research suggests that within large models, smaller subnetworks exist that, when properly trained, can achieve near-original performance.</p>
  </li>
</ul>

<h3 id="what-is-knowledge-distillation"><strong>What is Knowledge Distillation?</strong></h3>

<p>Distillation is the process of transferring knowledge from a larger, complex <strong>teacher model</strong> to a smaller <strong>student model</strong>. The goal is to make the student model mimic the teacher’s knowledge and behavior, reducing size without drastically losing performance.</p>

<p>There are two main approaches:</p>

<ol>
  <li>
    <p><strong>Synthetic Data (SGD) Fine-tuning:</strong> The teacher generates synthetic data, which is then used to fine-tune the student. The student only learns to predict the final output tokens.</p>
  </li>
  <li>
    <p><strong>Classical Knowledge Distillation:</strong> Instead of just mimicking the final output, the student also learns the teacher’s intermediate representations, logits, and embeddings. This provides richer supervision and leads to better generalization.</p>
  </li>
</ol>

<h4 id="variations-of-knowledge-distillation"><strong>Variations of Knowledge Distillation</strong></h4>

<ul>
  <li>
    <p><strong>Feature-based distillation:</strong> The student learns feature maps from the teacher to capture internal representations.</p>
  </li>
  <li>
    <p><strong>Contrastive distillation:</strong> The student is trained to differentiate between correct and incorrect outputs based on teacher insights.</p>
  </li>
  <li>
    <p><strong>Progressive distillation:</strong> Gradually reduces teacher complexity rather than training the student in a single step.</p>
  </li>
</ul>

<h3 id="how-to-prune-and-distill-an-llm"><strong>How to Prune and Distill an LLM</strong></h3>

<p>Here’s a step-by-step breakdown of how pruning and distillation work together:</p>

<ol>
  <li>
    <p><strong>Start with a large model (e.g., 15B parameters).</strong></p>
  </li>
  <li>
    <p><strong>Analyze importance:</strong> Rank and identify the least important layers using activation-based importance estimation on a small calibration dataset (~1024 samples).</p>
  </li>
  <li>
    <p><strong>Prune the model:</strong> Remove unimportant components based on the ranking.</p>
  </li>
  <li>
    <p><strong>Perform knowledge distillation:</strong> Use the original model as a teacher and the pruned model as a student.</p>
  </li>
  <li>
    <p><strong>Iterate:</strong> The pruned and distilled model can serve as a base for further pruning and distillation, progressively creating even smaller versions.</p>
  </li>
</ol>

<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*ce70Vw2XyY090HJs" alt="Diagram" /></p>

<hr />

<h2 id="best-practices-for-pruning-and-distillation"><strong>Best Practices for Pruning and Distillation</strong></h2>

<h3 id="model-sizing"><strong>Model Sizing</strong></h3>

<ul>
  <li>
    <p>If you plan to train multiple versions of an LLM, start with the largest model and <strong>iteratively prune and distill</strong> to obtain smaller versions.</p>
  </li>
  <li>
    <p>If the largest model was trained using multiple training phases, prune and distill from the final phase’s version.</p>
  </li>
</ul>

<h3 id="pruning-strategy"><strong>Pruning Strategy</strong></h3>

<ul>
  <li>
    <p><strong>Width pruning (removing neurons) is preferable over depth pruning (removing layers).</strong></p>
  </li>
  <li>
    <p><strong>Use single-shot importance estimation</strong>—iterative importance estimation does not yield better results.</p>
  </li>
</ul>

<h3 id="retraining-and-fine-tuning"><strong>Retraining and Fine-Tuning</strong></h3>

<ul>
  <li>
    <p>Instead of conventional training, <strong>use distillation exclusively</strong> for retraining pruned models.</p>
  </li>
  <li>
    <p>If the model’s depth has been significantly reduced, <strong>use logit + intermediate state + embedding distillation</strong> for better performance.</p>
  </li>
  <li>
    <p>If the model’s depth is largely retained, <strong>logit-only distillation</strong> is sufficient.</p>
  </li>
</ul>

<hr />

<h2 id="conclusion"><strong>Conclusion</strong></h2>

<p>By combining pruning and knowledge distillation, we can create <strong>smaller, faster, and cheaper</strong> language models without sacrificing much performance. These techniques make it feasible to deploy strong NLP models in resource-constrained environments, paving the way for a future where efficient AI is widely accessible.</p>

<p>As research in this area continues, expect to see even more refined techniques that push the boundaries of efficiency without compromising language understanding capabilities.</p>


<footer>
  This Article is <b>open source</b>. Noticed a typo? </br>
  Or something unclear? Send a PR :)
</footer>
</div>
  </main>

  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    <!--by <strong>Aditya Anil</strong>-->
  </small>

  <div class="container-fluid justify-content-center">

  

  

    
    
      

    
    
      

    
    

    
    <a class="social mx-1"  href="mailto:aditya8anil@gmail.com"
       style="color: #6c757d"
       onMouseOver="this.style.color='#db4437'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fas fa-envelope fa-1x"></i>
    </a>
  
  

    
    

    
    <a class="social mx-1"  href="https://www.facebook.com/aditya8anil"
       style="color: #6c757d"
       onMouseOver="this.style.color='#3b5998'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-facebook fa-1x"></i>
    </a>
  
  

    
    

    
    <a class="social mx-1"  href="https://www.github.com/skaarfacee"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-1x"></i>
    </a>
  
  

    
    

    
    <a class="social mx-1"  href="https://www.instagram.com/aaaddii__10"
       style="color: #6c757d"
       onMouseOver="this.style.color='#405de6'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-instagram fa-1x"></i>
    </a>
  
  

    
    

    
    <a class="social mx-1"  href="https://www.linkedin.com/in/aditya-anil"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-1x"></i>
    </a>
  
  

    
    

    
    <a class="social mx-1"  href="https://open.spotify.com/user/31oblpch73x3nnnrekndxanrtqvm"
       style="color: #6c757d"
       onMouseOver="this.style.color='#1db954'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-spotify fa-1x"></i>
    </a>
  
  

    
    

    
    <a class="social mx-1"  href="https://www.twitter.com/AdityaAnil19"
       style="color: #6c757d"
       onMouseOver="this.style.color='#1da1f2'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-twitter fa-1x"></i>
    </a>
  
  

</div>

</footer>
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Card Animation jQuery -->
<script src="/assets/js/card-animation.js"></script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>

</body>

</html>